

//./bin/spark-shell

import scala.collection.Map
import scala.collection.mutable.ArrayBuffer
import scala.util.Random

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.mllib.recommendation._
import org.apache.spark.rdd.RDD

import org.apache.spark.sql.Row;
import org.apache.spark.sql.types._

val rawUserArtistData = sc.textFile("/Users/juandavid/Documents/GuideSparkJuanDavid/MusicRecomender/musicData/user_artist_data.txt")

//one way to get stats from user id
rawUserArtistData.map(_.split(' ')(0).toDouble).stats()

val schema =
      StructType(
        Array(
          StructField("userid", LongType, true),
          StructField("itemid", LongType, true),
          StructField("value", LongType, true)
        )
      )

//val sqlContext = new org.apache.spark.sql.SQLContext(sc)
//impor sqlContext.implicits._
val rowRDD_userartist = rawUserArtistData.map(x=>x.split(" ")).map(p => Row(p(0).toLong,p(1).toLong,p(2).toLong))
val userartistDF = sqlContext.createDataFrame(rowRDD_userartist, schema)

userartistDF.describe().show()
//delete na values
userartistDF.na.drop().size



val rawArtistData = sc.textFile("/Users/juandavid/Documents/GuideSparkJuanDavid/MusicRecomender/musicData/artist_data.txt")

//skip bad data 
def buildArtistByID(rawArtistData: RDD[String]) =
    rawArtistData.flatMap { line =>
      val (id, name) = line.span(_ != '\t')
      if (name.isEmpty) {
        None
      } else {
        try {
          Some((id.toInt, name.trim))
        } catch {
          case e: NumberFormatException => None
        }
      }
    }

def buildArtistAlias(rawArtistAlias: RDD[String]): Map[Int,Int] =
    rawArtistAlias.flatMap { line =>
      val tokens = line.split('\t')
      if (tokens(0).isEmpty) {
        None
      } else {
        Some((tokens(0).toInt, tokens(1).toInt))
      }
    }.collectAsMap()


def buildRatings(rawUserArtistData: RDD[String],bArtistAlias: Broadcast[Map[Int,Int]]) = {
    rawUserArtistData.map { line =>
      val Array(userID, artistID, count) = line.split(' ').map(_.toInt)
      val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)
      Rating(userID, finalArtistID, count)
    }
  }

//create ALS model primera prueba

val rawArtistAlias = sc.textFile("/Users/juandavid/Documents/GuideSparkJuanDavid/MusicRecomender/musicData/artist_alias.txt")
val bArtistAlias = sc.broadcast(buildArtistAlias(rawArtistAlias))

val trainData = buildRatings(rawUserArtistData, bArtistAlias).cache()

val model = ALS.trainImplicit(trainData, 10, 5, 0.01, 1.0)

trainData.unpersist()

//test recommendation
val userID = 2093760
val recommendations = model.recommendProducts(userID, 5)
//print recomendations
recommendations.foreach(println)

val recommendedProductIDs = recommendations.map(_.product).toSet

val rawArtistsForUser = rawUserArtistData.map(_.split(' ')).filter { case Array(user,_,_) => user.toInt == userID }
val existingProducts = rawArtistsForUser.map { case Array(_,artist,_) => artist.toInt }.collect().toSet
val artistByID = buildArtistByID(rawArtistData)

//nombre de artistas que el usuario escuho
artistByID.filter { case (id, name) => existingProducts.contains(id) }.values.collect().foreach(println)
//nombre de artistas que se predijieron
artistByID.filter { case (id, name) => recommendedProductIDs.contains(id) }.values.collect().foreach(println)

def unpersist(model: MatrixFactorizationModel): Unit = {
    // At the moment, it's necessary to manually unpersist the RDDs inside the model
    // when done with it in order to make sure they are promptly uncached
    model.userFeatures.unpersist()
    model.productFeatures.unpersist()
  }
//unpersist the values of the model
unpersit(model)

//BETTER MODEL

//create functoin areaUnderCurve
def areaUnderCurve(
      positiveData: RDD[Rating],
      bAllItemIDs: Broadcast[Array[Int]],
      predictFunction: (RDD[(Int,Int)] => RDD[Rating])) = {
    // What this actually computes is AUC, per user. The result is actually something
    // that might be called "mean AUC".

    // Take held-out data as the "positive", and map to tuples
    val positiveUserProducts = positiveData.map(r => (r.user, r.product))
    // Make predictions for each of them, including a numeric score, and gather by user
    val positivePredictions = predictFunction(positiveUserProducts).groupBy(_.user)

    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of
    // small AUC problems, and it would be inefficient, when a direct computation is available.

    // Create a set of "negative" products for each user. These are randomly chosen
    // from among all of the other items, excluding those that are "positive" for the user.
    val negativeUserProducts = positiveUserProducts.groupByKey().mapPartitions {
      // mapPartitions operates on many (user,positive-items) pairs at once
      userIDAndPosItemIDs => {
        // Init an RNG and the item IDs set once for partition
        val random = new Random()
        val allItemIDs = bAllItemIDs.value
        userIDAndPosItemIDs.map { case (userID, posItemIDs) =>
          val posItemIDSet = posItemIDs.toSet
          val negative = new ArrayBuffer[Int]()
          var i = 0
          // Keep about as many negative examples per user as positive.
          // Duplicates are OK
          while (i < allItemIDs.size && negative.size < posItemIDSet.size) {
            val itemID = allItemIDs(random.nextInt(allItemIDs.size))
            if (!posItemIDSet.contains(itemID)) {
              negative += itemID
            }
            i += 1
          }
          // Result is a collection of (user,negative-item) tuples
          negative.map(itemID => (userID, itemID))
        }
      }
    }.flatMap(t => t)
    // flatMap breaks the collections above down into one big set of tuples

    // Make predictions on the rest:
    val negativePredictions = predictFunction(negativeUserProducts).groupBy(_.user)

    // Join positive and negative by user
    positivePredictions.join(negativePredictions).values.map {
      case (positiveRatings, negativeRatings) =>
        // AUC may be viewed as the probability that a random positive item scores
        // higher than a random negative one. Here the proportion of all positive-negative
        // pairs that are correctly ranked is computed. The result is equal to the AUC metric.
        var correct = 0L
        var total = 0L
        // For each pairing,
        for (positive <- positiveRatings;
             negative <- negativeRatings) {
          // Count the correctly-ranked pairs
          if (positive.rating > negative.rating) {
            correct += 1
          }
          total += 1
        }
        // Return AUC: fraction of pairs ranked correctly
        correct.toDouble / total
    }.mean() // Return mean AUC over users
  }

//predict famous artist to all the user
 def predictMostListened(sc: SparkContext, train: RDD[Rating])(allData: RDD[(Int,Int)]) = {
    val bListenCount =
      sc.broadcast(train.map(r => (r.product, r.rating)).reduceByKey(_ + _).collectAsMap())
    allData.map { case (user, product) =>
      Rating(user, product, bListenCount.value.getOrElse(product, 0.0))
    }
  }
  
val bArtistAlias = sc.broadcast(buildArtistAlias(rawArtistAlias))
val allData = buildRatings(rawUserArtistData, bArtistAlias)

// create train,test,validation
val Array(trainData, restData) = allData.randomSplit(Array(0.8, 0.2))
val Array(valdationData, testData) = restData.randomSplit(Array(0.5, 0.5))
trainData.cache()
valdationData.cache()
  

//predict most famous artist  
val allItemIDs = allData.map(_.product).distinct().collect()
val bAllItemIDs = sc.broadcast(allItemIDs)
val mostListenedAUC = areaUnderCurve(valdationData, bAllItemIDs, predictMostListened(sc, trainData))
println(mostListenedAUC)

//Grid Search in spark OJO, and nested loop
val evaluations =
      for (rank   <- Array(10, 20, 30);
           lambda <- Array(1.0, 0.01 ,0.0001);
           alpha  <- Array(1.0, 5.0, 10.0, 40.0,45.0))
      yield {
        val model = ALS.trainImplicit(trainData, rank, 10, lambda, alpha)
        val auc = areaUnderCurve(valdationData, bAllItemIDs, model.predict)
        unpersist(model)
        ((rank, lambda, alpha), auc)
      }

evaluations.sortBy(_._2).reverse.foreach(println)

//select best model and run auc in test data
val auc_test = areaUnderCurve(testData, bAllItemIDs, model.predict)

    
    
