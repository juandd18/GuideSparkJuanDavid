
##Line de comandos
//solo ejecutar si usted no tiene common-1.0.0.jar
//este jar posee la clase en java XmlInputFormat, se utilizar para
//leer xml files
$ cd common/
$ mvn package

//ejecutamos en la lineas de commandos
//mirar que se encuentra el jar en target y que este en 
//la carpeta de spark
$ spark-shell --jars target/common-1.0.0.jar,
target/breeze-viz_2.10-0.9.jar, 
target/breeze_2.10-0.9.jar, 
target/jcommon-1.0.16.jar, 
target/jfreechart-1.0.13.jar 


import com.cloudera.datascience.common.XmlInputFormat
import org.apache.spark.SparkContext
import org.apache.hadoop.io.{Text, LongWritable}
import org.apache.hadoop.conf.Configuration
import org.jfree.chart.axis.ValueAxis 
import breeze.linalg._ 
import breeze.plot._ 

//genera metodo que lee el o los archivos xml
//linea:  in.map(line => line._2.toString) es line._2 porque line._1 es un id unico
def loadMedline(sc: SparkContext, path: String) = {
    val conf = new Configuration()
    conf.set(XmlInputFormat.START_TAG_KEY, "<MedlineCitation ")
    conf.set(XmlInputFormat.END_TAG_KEY, "</MedlineCitation>")
    val in = sc.newAPIHadoopFile(path, classOf[XmlInputFormat],
    classOf[LongWritable], classOf[Text], conf)
    in.map(line => line._2.toString) 
}


import scala.xml._
//cargar los datos
val registrosRaw = loadMedline(sc,"/Users/juandavid/Documents/medline_data/*.xml")

//convierte cada elemento en un Elem de scala.xml(XML)
val registrosTodos = registrosRaw.map(XML.loadString)

/*
The \ operator only works on direct children of the node; if we execute elem \ "Mesh
Heading", the result is an empty NodeSeq. To extract nondirect children of a given
node, we need to use the \\ operator:
*/

// este comando (x \\ "ForeName") busca dentro cada subelemto not direct children "Elem (xml object)"
val nombre  = registrosTodos.map(x => (x \\ "ForeName").map(x=>x.text)).flatMap(x=>x).zipWithIndex.
map(x=> (x._2,x._1.toLowerCase.capitalize))
val apellido  = registrosTodos.map(x => (x \\ "LastName").map(x=>x.text)).flatMap(x=>x).zipWithIndex.
map(x=> (x._2,x._1.toLowerCase.capitalize))
//se utiliza zipWithIndex para poder hacer el join
val nombrecompleto = nombre.join(apellido).map(x=> x._2)
nombrecompleto.cache()

//mostar nombre unicos
nombrecompleto.distinct().count()

//mostar NO nombre unicos
nombrecompleto.count()

//guardar archivo de nombre unicos. coalesce trae solo un formato se ejecuta en un worker no mas
nombrecompleto.distinct().coalesce(1,true).saveAsTextFile("/Users/juandavid/Documents/medline_data/nombresMedline")

//countByValue genera cuantas veces se repite un nombre 
//en este case significa cuantas veces un autor aparece 
//en varios documentos
val countNombre = nombrecompleto.countByValue()

//encontrar mean
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}

val dataforstats = sc.parallelize(countNombre.values.map(x=> Vectors.dense(x.toDouble)).toSeq)
val summary: MultivariateStatisticalSummary = Statistics.colStats(dataforstats)
// a dense vector containing the mean value for each column
summary.mean 
// como se puede ver no es poisson porque los valores 
//son muy distintos
summary.variance 


//Se pasa a una secuencia para luego ordernar
val countNombreSeq = countNombre.toSeq
//imprime los 20 primeros autores
countNombreSeq.sortBy(_._2).reverse.take(20).foreach(println)

//se crea histograma o distribucion 
val valueDist = countNombre.groupBy(_._2).mapValues(_.size)
valueDist.toSeq.sorted.take(10).foreach(println)


//importar para hacer plot y kernel density
import org.apache.spark.mllib.stat.KernelDensity
import org.apache.spark.rdd.RDD

//function that creates kernel density
def plotDistributionKernel(samples: Map[Long,Int]) {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    val domain = Range.Double(min, max, (max - min) / 100).
    toList.toArray
    val values = samples.values.map(x=>x.toDouble).toArray
    val densities = KernelDensity.estimate(values, domain)
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain, densities)
    p.xlabel = "data"
    p.ylabel ="y"
}

//plot
plotDistributionKernel(valueDist)

def plotDistribution(samples: Map[Long,Int]) {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    
    val domain = samples.toArray.sortBy(_._1).map(x=>x._1.toDouble)
    val values = samples.toArray.sortBy(_._1).map(x=> (x._2.toDouble))
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain,values, '.')
    p.title = "data"
}

//function that plot density
def plotDistributionStd(samples: Map[Long,Int]) {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    
    val domain = samples.toArray.sortBy(_._1).map(x=>x._1.toDouble)
    val values = samples.toArray.sortBy(_._1).map(x=> (x._2.toDouble - min)/(max - min)  )
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain,values, '.')
    p.title = "data standarize"
}

//fit a distribution to valueDist
//como se puede observar en el plot anterior 
//puede ser una poisson o chi square

//miramos se es poisson en R
/****CODIGO EN RSTUDIO, copiar valueDist.value (ctrl + c) y pegar en rstudio
//Le falta un poco para que poisson sea pero es bastante cerca
library(fitdistrplus)
library(logspline)

data <-  c(1218, 159, 11, 3, 14, 1, 53, 22, 6, 576550, 1,
     718, 6, 2, 15, 7, 1, 219, 72, 2, 1, 22889, 8,
     5, 34, 19, 2, 1, 8, 1, 93, 1, 458, 5627, 6,
     1, 1, 27, 1, 1, 31, 3, 123, 7, 8, 1, 7,
     290, 1, 5, 9, 1, 21, 2255, 3, 51)

descdist(data, discrete = TRUE)


fit.logistic <- fitdist(data, "pois")
plot(fit.logistic)

******/

//mirar si es poisson en Spark

//vamos a generar poisson datos random
import org.apache.spark.mllib.random.RandomRDDs._

//the 10912.14 viene de summary.mean y 611080 countNombre.size
val datosPoisson = Vectors.dense(poissonRDD(sc, 1.11, 611080, seed=1L).toArray)
val vectdatos = Vectors.dense(countNombre.values.map(x=>x.toDouble).toArray)

//chiSqTest(observed: Vector, expected: Vector)
val goodnessOfFitTestResult = Statistics.chiSqTest(vectdatos,datosPoisson)
/* Null Hyphotesis : observed follows the same distribution as expected, p-value < 0.05 reject null hypothesis
//pero no indica que tipo de asociacion
ChiSqTest: Chi-squared approximation may not be accurate due to low expected frequencies  as a result of a large number of categories: 611080.
goodnessOfFitTestResult: org.apache.spark.mllib.stat.test.ChiSqTestResult = 
Chi squared test summary:
method: pearson
degrees of freedom = 611079 
statistic = Infinity 
pValue = 0.0 
Very strong presumption against null hypothesis: observed follows the same distribution as expected..
*/

val testResult = Statistics.kolmogorovSmirnovTest(sc.parallelize(countNombre.values.map(x=>x.toDouble).toSeq),
    "norm", 1.15, 5.93)
/* Null Hypothesis: the sample is drawn for that distribution, p-value < 0.05 reject null hypothesis
Kolmogorov-Smirnov test summary:
degrees of freedom = 0 
statistic = 0.48990978732440305 
pValue = 1.9159279718294897E-9 
Very strong presumption against null hypothesis: Sample follows theoretical distribution.
*/

//LOS DATOS NO SON POISSON NO NORMAL COMO LO COMPRUEBA R (graficos) y los hypothesis testing
//Tenemos que hacer MCMC 

//hallar conjugate prior of poisson










