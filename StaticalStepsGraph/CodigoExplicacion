
##Line de comandos
//solo ejecutar si usted no tiene common-1.0.0.jar
//este jar posee la clase en java XmlInputFormat, se utilizar para
//leer xml files
$ cd common/
$ mvn package

//ejecutamos en la lineas de commandos
//mirar que se encuentra el jar en target y que este en 
//la carpeta de spark
$ spark-shell --jars target/common-1.0.0.jar,
target/breeze-viz_2.10-0.9.jar, 
target/breeze_2.10-0.9.jar, 
target/jcommon-1.0.16.jar, 
target/jfreechart-1.0.13.jar 
target/guava-18.0.jar

import com.cloudera.datascience.common.XmlInputFormat
import org.apache.spark.SparkContext
import org.apache.hadoop.io.{Text, LongWritable}
import org.apache.hadoop.conf.Configuration
import org.jfree.chart.axis.ValueAxis 
import breeze.linalg._ 
import breeze.plot._ 

//genera metodo que lee el o los archivos xml
//linea:  in.map(line => line._2.toString) es line._2 porque line._1 es un id unico
def loadMedline(sc: SparkContext, path: String) = {
    val conf = new Configuration()
    conf.set(XmlInputFormat.START_TAG_KEY, "<MedlineCitation ")
    conf.set(XmlInputFormat.END_TAG_KEY, "</MedlineCitation>")
    val in = sc.newAPIHadoopFile(path, classOf[XmlInputFormat],
    classOf[LongWritable], classOf[Text], conf)
    in.map(line => line._2.toString) 
}


import scala.xml._
//cargar los datos
val registrosRaw = loadMedline(sc,"/Users/juandavid/Documents/medline_data/*.xml")

//convierte cada elemento en un Elem de scala.xml(XML)
val registrosTodos = registrosRaw.map(XML.loadString)

/*
The \ operator only works on direct children of the node; if we execute elem \ "Mesh
Heading", the result is an empty NodeSeq. To extract nondirect children of a given
node, we need to use the \\ operator:
*/

// este comando (x \\ "ForeName") busca dentro cada subelemto not direct children "Elem (xml object)"
val nombre  = registrosTodos.map(x => (x \\ "ForeName").map(x=>x.text)).flatMap(x=>x).zipWithIndex.
map(x=> (x._2,x._1.toLowerCase.capitalize))
val apellido  = registrosTodos.map(x => (x \\ "LastName").map(x=>x.text)).flatMap(x=>x).zipWithIndex.
map(x=> (x._2,x._1.toLowerCase.capitalize))
//se utiliza zipWithIndex para poder hacer el join
val nombrecompleto = nombre.join(apellido).map(x=> (x._2._1 + " "+ x._2._2) )
nombrecompleto.cache()

//mostar nombre unicos
nombrecompleto.distinct().count()

//mostar NO nombre unicos
nombrecompleto.count()

//guardar archivo de nombre unicos. coalesce trae solo un formato se ejecuta en un worker no mas
nombrecompleto.distinct().coalesce(1,true).saveAsTextFile("/Users/juandavid/Documents/medline_data/nombresMedline")

//countByValue genera cuantas veces se repite un nombre 
//en este case significa cuantas veces un autor aparece 
//en varios documentos
val countNombre = nombrecompleto.countByValue()

//encontrar mean
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}

val dataforstats = sc.parallelize(countNombre.values.map(x=> Vectors.dense(x.toDouble)).toSeq)
val summary: MultivariateStatisticalSummary = Statistics.colStats(dataforstats)
// a dense vector containing the mean value for each column
summary.mean 
// como se puede ver no es poisson porque los valores 
//son muy distintos
summary.variance 


//Se pasa a una secuencia para luego ordernar
val countNombreSeq = countNombre.toSeq
//imprime los 20 primeros autores
countNombreSeq.sortBy(_._2).reverse.take(20).foreach(println)

//se crea histograma o distribucion 
val valueDist = countNombre.groupBy(_._2).mapValues(_.size)
valueDist.toSeq.sorted.take(10).foreach(println)


//importar para hacer plot y kernel density
import org.apache.spark.mllib.stat.KernelDensity
import org.apache.spark.rdd.RDD

//function that creates kernel density
def plotDistributionKernel(samples: Map[Long,Int]) {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    val domain = Range.Double(min, max, (max - min) / 100).
    toList.toArray
    val values = samples.values.map(x=>x.toDouble).toArray
    val densities = KernelDensity.estimate(values, domain)
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain, densities)
    p.xlabel = "data"
    p.ylabel ="y"
}

//plot
plotDistributionKernel(valueDist)

def plotDistribution(samples: Map[Long,Int]) {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    
    val domain = samples.toArray.sortBy(_._1).map(x=>x._1.toDouble)
    val values = samples.toArray.sortBy(_._1).map(x=> (x._2.toDouble))
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain,values, '.')
    p.title = "data"
}

//function that plot density
def plotDistributionStd(samples: Map[Long,Int]) {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    
    val domain = samples.toArray.sortBy(_._1).map(x=>x._1.toDouble)
    val values = samples.toArray.sortBy(_._1).map(x=> (x._2.toDouble - min)/(max - min)  )
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain,values, '.')
    p.title = "data standarize"
}

//fit a distribution to valueDist
//como se puede observar en el plot anterior 
//puede ser una poisson o chi square

//miramos se es poisson en R
/****CODIGO EN RSTUDIO, saveAsTextFile countNombre y cargar en rsutio
//Le falta un poco para que poisson sea pero es bastante cerca
library(fitdistrplus)
library(logspline)

part.00000 <- read.table("~/Documents/medline_data/datafinal/part-00000", quote="\"", comment.char="", stringsAsFactors=FALSE)

numeric_med <- as.integer(part.00000$V1)

descdist(numeric_med, discrete = FALSE )

fit.logistic <- fitdist(numeric_med, "pois")
plot(fit.logistic)

******/

//mirar si es poisson en Spark

//vamos a generar poisson datos random
import org.apache.spark.mllib.random.RandomRDDs._

//the 10912.14 viene de summary.mean y 611080 countNombre.size
val datosPoisson = Vectors.dense(poissonRDD(sc, 1.11, 611080, seed=1L).toArray)
val vectdatos = Vectors.dense(countNombre.values.map(x=>x.toDouble).toArray)

//chiSqTest(observed: Vector, expected: Vector)
val goodnessOfFitTestResult = Statistics.chiSqTest(vectdatos,datosPoisson)
/* Null Hyphotesis : observed follows the same distribution as expected, p-value < 0.05 reject null hypothesis
//pero no indica que tipo de asociacion
ChiSqTest: Chi-squared approximation may not be accurate due to low expected frequencies  as a result of a large number of categories: 611080.
goodnessOfFitTestResult: org.apache.spark.mllib.stat.test.ChiSqTestResult = 
Chi squared test summary:
method: pearson
degrees of freedom = 611079 
statistic = Infinity 
pValue = 0.0 
Very strong presumption against null hypothesis: observed follows the same distribution as expected..
*/

val testResult = Statistics.kolmogorovSmirnovTest(sc.parallelize(countNombre.values.map(x=>x.toDouble).toSeq),
    "norm", 1.15, 5.93)
/* Null Hypothesis: the sample is drawn for that distribution, p-value < 0.05 reject null hypothesis
Kolmogorov-Smirnov test summary:
degrees of freedom = 0 
statistic = 0.48990978732440305 
pValue = 1.9159279718294897E-9 
Very strong presumption against null hypothesis: Sample follows theoretical distribution.
*/

//LOS DATOS NO SON POISSON NO NORMAL COMO LO COMPRUEBA R (graficos) y los hypothesis testing
//Tenemos que hacer MCMC FALTA POR HACER OJOOOOOOOOOOOO


//Continuamos
//Vamos a mostrar cuales pares de doctores escriben mas seguido

val doctorPairs = nombrecompleto.distinct()
val doctorPariCart = doctorPairs.cartesian(doctorPairs)

val doctorPairsmap = doctorPariCart.map(p=>(p,1))
val cooccurs = doctorPairsmap.reduceByKey(_+_)
val cooccursSorted = cooccurs.sortBy(_._2)
cooccursSorted.take(10)

//Vamos a crear Graph de medline (co ocurrencias)

import java.security.{MessageDigest, NoSuchAlgorithmException}
import java.nio.charset.StandardCharsets

def hashId(str: String): Long = {
    // This is effectively the same implementation as in Guava's Hashing, but 'inlined'
    // to avoid a dependency on Guava just for this. It creates a long from the first 8 bytes
    // of the (16 byte) MD5 hash, with first byte as least-significant byte in the long.
    val bytes = MessageDigest.getInstance("MD5").digest(str.getBytes(StandardCharsets.UTF_8))
    (bytes(0) & 0xFFL) |
    ((bytes(1) & 0xFFL) << 8) |
    ((bytes(2) & 0xFFL) << 16) |
    ((bytes(3) & 0xFFL) << 24) |
    ((bytes(4) & 0xFFL) << 32) |
    ((bytes(5) & 0xFFL) << 40) |
    ((bytes(6) & 0xFFL) << 48) |
    ((bytes(7) & 0xFFL) << 56)
  }

val vertices = nombrecompleto.distinct().map(nombre => (hashId(nombre), nombre))

//verificar que los hash values son unicos
val uniqueHashes = vertices.map(_._1).countByValue()
val uniqueTopics = vertices.map(_._2).countByValue()
uniqueHashes.size == uniqueTopics.size


import org.apache.spark.graphx._

//crear edges 
val edges = cooccurs.map(p => {
    val (nombres, cnt) = p
    val ids = nombres.map(hashId).sorted
    Edge(ids(0), ids(1), cnt)
})










