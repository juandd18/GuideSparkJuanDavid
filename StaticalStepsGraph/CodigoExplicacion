
##Line de comandos
//solo ejecutar si usted no tiene common-1.0.0.jar
//este jar posee la clase en java XmlInputFormat, se utilizar para
//leer xml files
$ cd common/
$ mvn package

//ejecutamos en la lineas de commandos
//mirar que se encuentra el jar en target y que este en 
//la carpeta de spark
$ spark-shell --jars target/common-1.0.0.jar,
target/breeze-viz_2.10-0.9.jar, 
target/breeze_2.10-0.9.jar, 
target/jcommon-1.0.16.jar, 
target/jfreechart-1.0.13.jar 


import com.cloudera.datascience.common.XmlInputFormat
import org.apache.spark.SparkContext
import org.apache.hadoop.io.{Text, LongWritable}
import org.apache.hadoop.conf.Configuration
import org.jfree.chart.axis.ValueAxis 
import breeze.linalg._ 
import breeze.plot._ 

//genera metodo que lee el o los archivos xml
//linea:  in.map(line => line._2.toString) es line._2 porque line._1 es un id unico
def loadMedline(sc: SparkContext, path: String) = {
    val conf = new Configuration()
    conf.set(XmlInputFormat.START_TAG_KEY, "<MedlineCitation ")
    conf.set(XmlInputFormat.END_TAG_KEY, "</MedlineCitation>")
    val in = sc.newAPIHadoopFile(path, classOf[XmlInputFormat],
    classOf[LongWritable], classOf[Text], conf)
    in.map(line => line._2.toString) 
}


import scala.xml._
//cargar los datos
val registrosRaw = loadMedline(sc,"/Users/juandavid/Documents/medline_data/*.xml")

//convierte cada elemento en un Elem de scala.xml(XML)
val registrosTodos = registrosRaw.map(XML.loadString)

/*
The \ operator only works on direct children of the node; if we execute elem \ "Mesh
Heading", the result is an empty NodeSeq. To extract nondirect children of a given
node, we need to use the \\ operator:
*/

// este comando (x \\ "ForeName") busca dentro cada subelemto not direct children "Elem (xml object)"
val nombre  = registrosTodos.map(x => (x \\ "ForeName").map(x=>x.text)).flatMap(x=>x).zipWithIndex.
map(x=> (x._2,x._1.toUpperCase.capitalize))
val apellido  = registrosTodos.map(x => (x \\ "LastName").map(x=>x.text)).flatMap(x=>x).zipWithIndex.
map(x=> (x._2,x._1.toLowerCase.capitalize))
//se utiliza zipWithIndex para poder hacer el join
val nombrecompleto = nombre.join(apellido).map(x=> x._2)
nombrecompleto.cache()

//mostar nombre unicos
nombrecompleto.distinct().count()

//mostar NO nombre unicos
nombrecompleto.count()

//guardar archivo de nombre unicos. coalesce trae solo un formato se ejecuta en un worker no mas
nombrecompleto.distinct().coalesce(1,true).saveAsTextFile("/Users/juandavid/Documents/medline_data/nombresMedline")

//countByValue genera cuantas veces se repite un nombre 
//en este case significa cuantas veces un autor aparece 
//en varios documentos
val countNombre = nombrecompleto.countByValue()


//encontrar mean
val mean = countNombre.values.sum/countNombre.size

//encontrar desviacion standar y varianza
val variance = countNombre.values.map(x=> (x - mean) * (x - mean))
val stddev = Math.sqrt(variance.sum.toDouble / countNombre.size.toDouble)

//Se pasa a una secuencia para luego ordernar
val countNombreSeq = countNombre.toSeq
//imprime los 20 primeros autores
countNombreSeq.sortBy(_._2).reverse.take(20).foreach(println)

//se crea histograma o distribucion 
val valueDist = countNombre.groupBy(_._2).mapValues(_.size)
valueDist.toSeq.sorted.take(10).foreach(println)

//importar para hacer plot y kernel density
import org.apache.spark.mllib.stat.KernelDensity
import org.apache.spark.rdd.RDD

//function that creates kernel density
def plotDistribution(samples: Array[Double]) {
    val min = samples.min
    val max = samples.max
    val domain = Range.Double(min, max, (max - min) / 100).
    toList.toArray
    val densities = KernelDensity.estimate(samples, domain)
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain, densities)
    p.xlabel = "data"
    p.ylabel ="y"
}

//plot
plotDistribution(valueDist.values.map(x=>x.toDouble).toSeq.sorted.toArray)






