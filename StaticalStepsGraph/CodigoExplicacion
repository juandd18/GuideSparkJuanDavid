/*CREAR CLUSTER EN ec2
./spark-ec2 --key-pair=ClouderaEc2 --identity-file=/Users/juandavid/Documents/awsSet/ClouderaEc2.pem --region=us-east-1 --instance-type=m3.large -s 1 --copy-aws-credentials --hadoop-major-version=2 launch juan-cluster
*/


/* Para ejecutar spark en order los jars
./bin/spark-shell --jars /Users/juandavid/Documents/GuideSparkJuanDavid/StaticalStepsGraph/common-1.0.2.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/StaticalStepsGraph/breeze-viz_2.10-0.9.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/StaticalStepsGraph/breeze_2.10-0.9.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/StaticalStepsGraph/jcommon-1.0.16.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/StaticalStepsGraph/jfreechart-1.0.13.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/StaticalStepsGraph/guava-18.0.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/StaticalStepsGraph/ch09-risk-1.0.2.jar
bin/spark-shell --jars common-1.0.2.jar,breeze-viz_2.10-0.9.jar,breeze_2.10-0.9.jar,jcommon-1.0.16.jar,jfreechart-1.0.13.jar,
*/
##Line de comandos
//solo ejecutar si usted no tiene common-1.0.0.jar
//este jar posee la clase en java XmlInputFormat, se utilizar para
//leer xml files
$ cd common/
$ mvn package

//ejecutamos en la lineas de commandos
//mirar que se encuentra el jar en target y que este en 
//la carpeta de spark
$ spark-shell --jars target/common-1.0.0.jar,
target/breeze-viz_2.10-0.9.jar, 
target/breeze_2.10-0.9.jar, 
target/jcommon-1.0.16.jar, 
target/jfreechart-1.0.13.jar 
target/guava-18.0.jar

import com.cloudera.datascience.common.XmlInputFormat

import lectura.juanDos.XmlInputFormat

import org.apache.spark.SparkContext
import org.apache.hadoop.io.{Text, LongWritable}
import org.apache.spark.rdd.RDD

import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapred.TextInputFormat

import org.apache.commons.math3.distribution._

import org.apache.hadoop.conf.Configuration
import org.jfree.chart.axis.ValueAxis 
import breeze.linalg._ 
import breeze.plot._ 

val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.implicits._

//genera metodo que lee el o los archivos xml
//linea:  in.map(line => line._2.toString) es line._2 porque line._1 es un id unico
def loadMedline(sc: SparkContext, path: String) = {
    val conf = new Configuration()
    conf.set(XmlInputFormat.START_TAG_KEY, "<MedlineCitation ")
    conf.set(XmlInputFormat.END_TAG_KEY, "</MedlineCitation>")
    val in = sc.newAPIHadoopFile(path, classOf[XmlInputFormat],
    classOf[LongWritable], classOf[Text], conf)
    in.map(line => line._2.toString) 
}



//val registrosRaw = loadMedline(sc,"/Users/juandavid/Documents/medline_data/medsamp2015a.xml")

//val path = "hdfs://ec2-54-237-83-252.compute-1.amazonaws.com:9000/medsamp2015a.xml"


val registrosRaw = loadMedline(sc,path)
//val registrosRaw = loadMedline(sc,"/Users/juandavid/Documents/medline_data/*.xml")

import scala.xml._
//cargar los datos
//convierte cada elemento en un Elem de scala.xml(XML)
val registrosTodos: RDD[Elem]  = registrosRaw.map(XML.loadString)

/*
The \ operator only works on direct children of the node; if we execute elem \ "Mesh
Heading", the result is an empty NodeSeq. To extract nondirect children of a given
node, we need to use the \\ operator:
*/

//FALTA ARREGLAR CAPITALIZACION
val nombre  = registrosTodos.map(x => ((x \\ "ForeName").map(x=>x.text) zip (x \\ "LastName").map(x=>x.text))  )
val nombreListado = nombre.map(x=> x.map(x => x._1.toLowerCase.capitalize + " " +x._2.toLowerCase.capitalize ) )
val nombrecompleto = nombreListado.flatMap(x=>x)

nombreListado.cache()
nombreListado.take(1)

nombrecompleto.cache()

//mostar nombre unicos
nombrecompleto.distinct().count()

//mostar NO nombre unicos
nombrecompleto.count()

//guardar archivo de nombre unicos. coalesce trae solo un formato se ejecuta en un worker no mas
nombrecompleto.distinct().coalesce(1,true).saveAsTextFile("/Users/juandavid/Documents/medline_data/nombresMedline")

//countByValue genera cuantas veces se repite un nombre 
//en este case significa cuantas veces un autor aparece 
//en varios documentos
val countNombre = nombrecompleto.countByValue()

//encontrar mean
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}

val dataforstats = sc.parallelize(countNombre.values.map(x=> Vectors.dense(x.toDouble)).toSeq)
val summary: MultivariateStatisticalSummary = Statistics.colStats(dataforstats)
// a dense vector containing the mean value for each column
summary.mean 
// como se puede ver no es poisson porque los valores 
//son muy distintos
summary.variance 


//Se pasa a una secuencia para luego ordernar
val countNombreSeq = countNombre.toSeq
//imprime los 20 primeros autores
countNombreSeq.sortBy(_._2).reverse.take(20).foreach(println)

//se crea histograma o distribucion 
val valueDist = countNombre.groupBy(_._2).mapValues(_.size)
valueDist.toSeq.sorted.take(10).foreach(println)


//importar para hacer plot y kernel density
import org.apache.spark.mllib.stat.KernelDensity
import org.apache.spark.rdd.RDD

//function that creates kernel density
def plotDistributionKernel(samples: Map[Long,Int]) {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    val domain = Range.Double(min, max, (max - min) / 100).
    toList.toArray
    val values = samples.values.map(x=>x.toDouble).toArray
    val densities = KernelDensity.estimate(values, domain)
    
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain, densities)
    p.xlabel = "data"
    p.ylabel ="y"
}

//plot
plotDistributionKernel(valueDist)

def plotDistribution(samples: Map[Long,Int]) {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    
    val domain = samples.toArray.sortBy(_._1).map(x=>x._1.toDouble)
    val values = samples.toArray.sortBy(_._1).map(x=> (x._2.toDouble))
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain,values, '.')
    p.title = "data"
}

//function that plot density
def plotDistributionStd(samples: Map[Long,Int]) {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    
    val domain = samples.toArray.sortBy(_._1).map(x=>x._1.toDouble)
    val values = samples.toArray.sortBy(_._1).map(x=> (x._2.toDouble - min)/(max - min)  )
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain,values, '.')
    p.title = "data standarize"
}

def plotDistributionStd2(samples: scala.collection.Map[Long,Int]) = {
    val min = samples.values.map(x=>x.toDouble).min
    val max = samples.values.map(x=>x.toDouble).max
    
    val domain = samples.toArray.sortBy(_._1).map(x=>x._1.toDouble)
    val values = samples.toArray.sortBy(_._1).map(x=> (x._2.toDouble))
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain,values, '.')
    p.title = "data"
}

import org.apache.commons.math3.distribution._

val zip = new ZipfDistribution(30,1.9)
val zip_valores = sc.parallelize(zip.sample(33050))
val zip_Dist = zip_valores.countByValue().map(x=> (x._2,x._1))
plotDistributionStd2(zip_Dist)

val datoszip = org.apache.spark.mllib.linalg.Vectors.dense(zip_valores.map(x => x.toDouble).toArray)
val vectdatos = Vectors.dense(countNombre.values.map(x=>x.toDouble).toArray)
val goodnessOfFitTestResult = Statistics.chiSqTest(vectdatos,datoszip)
/*
Chi squared test summary:
method: pearson
degrees of freedom = 33049 
statistic = 93953.88969149816 
pValue = 0.0 
Very strong presumption against null hypothesis: observed follows the same distribution as expected..
*/

//fit a distribution to valueDist
//como se puede observar en el plot anterior 
//puede ser una poisson o chi square

//miramos se es poisson en R
/****CODIGO EN RSTUDIO, saveAsTextFile countNombre y cargar en rsutio
//Le falta un poco para que poisson sea pero es bastante cerca
library(fitdistrplus)
library(logspline)

part.00000 <- read.table("~/Documents/medline_data/datafinal/part-00000", quote="\"", comment.char="", stringsAsFactors=FALSE)

numeric_med <- as.integer(part.00000$V1)

descdist(numeric_med, discrete = FALSE )

fit.logistic <- fitdist(numeric_med, "pois")
plot(fit.logistic)

******/

//mirar si es poisson en Spark

//vamos a generar poisson datos random
import org.apache.spark.mllib.random.RandomRDDs._

//the 10912.14 viene de summary.mean y 611080 countNombre.size
val datosPoisson = Vectors.dense(poissonRDD(sc, 1.11, 611080, seed=1L).toArray)
val vectdatos = Vectors.dense(countNombre.values.map(x=>x.toDouble).toArray)

//chiSqTest(observed: Vector, expected: Vector)
val goodnessOfFitTestResult = Statistics.chiSqTest(vectdatos,datosPoisson)
/* Null Hyphotesis : observed follows the same distribution as expected, p-value < 0.05 reject null hypothesis
//pero no indica que tipo de asociacion
ChiSqTest: Chi-squared approximation may not be accurate due to low expected frequencies  as a result of a large number of categories: 611080.
goodnessOfFitTestResult: org.apache.spark.mllib.stat.test.ChiSqTestResult = 
Chi squared test summary:
method: pearson
degrees of freedom = 611079 
statistic = Infinity 
pValue = 0.0 
Very strong presumption against null hypothesis: observed follows the same distribution as expected..
*/

val testResult = Statistics.kolmogorovSmirnovTest(sc.parallelize(countNombre.values.map(x=>x.toDouble).toSeq),
    "norm", 1.15, 5.93)
/* Null Hypothesis: the sample is drawn for that distribution, p-value < 0.05 reject null hypothesis
Kolmogorov-Smirnov test summary:
degrees of freedom = 0 
statistic = 0.48990978732440305 
pValue = 1.9159279718294897E-9 
Very strong presumption against null hypothesis: Sample follows theoretical distribution.
*/

//LOS DATOS NO SON POISSON NO NORMAL COMO LO COMPRUEBA R (graficos) y los hypothesis testing
//Tenemos que hacer MCMC FALTA POR HACER OJOOOOOOOOOOOO


//Continuamos
//Vamos a mostrar cuales pares de doctores escriben mas seguido

val nombrePairs = nombreListado.flatMap(t => t.sorted.combinations(2))
val cooccurs = nombrePairs.map(p => (p, 1)).reduceByKey(_+_)
cooccurs.cache()
cooccurs.count()

//ordena descendentemente por la segunda columna
val cooccursSorted = cooccurs.sortBy(_._2,false)
cooccursSorted.take(10)



//Vamos a crear Graph de medline (co ocurrencias)

import java.security.{MessageDigest, NoSuchAlgorithmException}
import java.nio.charset.StandardCharsets

def hashId(str: String): Long = {
    // This is effectively the same implementation as in Guava's Hashing, but 'inlined'
    // to avoid a dependency on Guava just for this. It creates a long from the first 8 bytes
    // of the (16 byte) MD5 hash, with first byte as least-significant byte in the long.
    val bytes = MessageDigest.getInstance("MD5").digest(str.getBytes(StandardCharsets.UTF_8))
    (bytes(0) & 0xFFL) |
    ((bytes(1) & 0xFFL) << 8) |
    ((bytes(2) & 0xFFL) << 16) |
    ((bytes(3) & 0xFFL) << 24) |
    ((bytes(4) & 0xFFL) << 32) |
    ((bytes(5) & 0xFFL) << 40) |
    ((bytes(6) & 0xFFL) << 48) |
    ((bytes(7) & 0xFFL) << 56)
  }

val nombrevertices = nombrecompleto.distinct().cache()

val vertices = nombrevertices.map(nombre => (hashId(nombre), nombre))

//verificar que los hash values son unicos
val uniqueHashes = vertices.map(_._1).countByValue()
val uniqueTopics = vertices.map(_._2).countByValue()
uniqueHashes.size == uniqueTopics.size


import org.apache.spark.graphx._

//crear edges 
val edges = cooccurs.map(p => {
    val (nombres, cnt) = p
    val ids = nombres.map(hashId).sorted
    Edge(ids(0), ids(1), cnt)
})

//crear graph
val doctorGraph = Graph(vertices, edges)
doctorGraph.cache()

//Connected Components
val connectedComponentGraph: Graph[VertexId, Int] = doctorGraph.connectedComponents()

/*get a count of the number of connected components and their size, we can use the
trusty countByValue method against the VertexId values for each vertex in the Ver
texRDD. Weâ€™ll write a function to find a list of all the connected components, sorted by
their sizes
*/
def sortedConnectedComponents(connectedComponents: Graph[VertexId, _]) : Seq[(VertexId, Long)] = {
val componentCounts = connectedComponents.vertices.map(_._2).
countByValue
componentCounts.toSeq.sortBy(_._2).reverse
}

//how many components there are 
val componentCounts = sortedConnectedComponents(connectedComponentGraph)
componentCounts.size

//show the 10 bigest components
componentCounts.take(10).foreach(println)

//find the names of the doctors for each component by a inner join 
//with the doctorGraph and connectedComponents
val doctornameCID = doctorGraph.vertices.innerJoin(connectedComponentGraph.vertices) {
(doctorId, name, componentId) => (name, componentId)
}

//show names of doctor which are not in the largest component 
//componentCounts(1)._1 is the second largest component
val c1 = doctornameCID.filter(x => x._2._2 == componentCounts(1)._1)
c1.take(5).foreach(x => println(x._2._1))

//how many times R Joyeuse writes a document
val R_Joyeuse = nombrecompleto.filter(_.contains("R Joyeuse")).countByValue()
R_Joyeuse.take(1)

//DEGREES
val degrees: VertexRDD[Int] = doctorGraph.degrees.cache()

//show degree stats
//Estos datos nos muestran que hay mas vertices que degrees es decir
//hay edges o doctores que escribieron articulos solos
degrees.map(_._2).stats()
//(count: 21341, mean: 2,266342, stdev: 1,846737, max: 26,000000, min: 1,000000)

doctorGraph.vertices.count()
//res57: Long = 33050

//vamos a encontrar cuantos doctores escribieron alguna vez un
//articulo solos en este case existen 14281 doctores
val sing = nombreListado.filter(x => x.size == 1)
val singDoctor = sing.flatMap(doctor => doctor).distinct()
singDoctor.count()
//res58: Long = 14281

//ahora vamos a encontrar los doctores que solo 
//han escritos solos (11709)
val topic2 = nombrePairs.flatMap(p => p)
singDoctor.subtract(topic2).count()
//res60: Long = 11709

//get the names of the top degree vertices
def topNamesAndDegrees(degrees: VertexRDD[Int],doctorGraph: Graph[String, Int]): Array[(String, Int)] = {
val namesAndDegrees = degrees.innerJoin(doctorGraph.vertices) {
(topicId, degree, name) => (name, degree)
}
namesAndDegrees.map(_._2).sortBy(_._2,false).take(10)

}

topNamesAndDegrees(degrees, doctorGraph).foreach(println)
//(A v Hardy,26) (D Philippides,22)....


//Processing EdgeTriplets Chisquare

val T = nombrecompleto.count()

val verticesRddCount = nombrevertices.map(nombre => (hashId(nombre), 1)).reduceByKey(_ + _)

val nombreCountGraph = Graph(verticesRddCount, doctorGraph.edges)

//calculate chisquare
def chiSq(YY: Int, YB: Int, YA: Int, T: Long): Double = {
val NB = T - YB
val NA = T - YA
val YN = YA - YY
val NY = YB - YY
val NN = T - NY - YN - YY
val inner = math.abs(YY * NN - YN * NY) - T / 2.0
T * math.pow(inner, 2) / (YA * NA * YB * NB)
}

val chiSquaredGraph = nombreCountGraph.mapTriplets(triplet => {
chiSq(triplet.attr, triplet.srcAttr, triplet.dstAttr, T)
})
chiSquaredGraph.edges.map(x => x.attr).stats()
//(count: 24183, mean: 36592,784980, stdev: 402338,358111, max: 40359525,303775, min: 11593,249995)

//obtiene un subgraph con solo los vertices que 
//cumplen la distribucion chi square 
val interesting = chiSquaredGraph.subgraph(triplet => triplet.attr > 19.5)

//cuantos edges quedaron
interesting.edges.count
//24183

//vamos a realizar los mismos analisis de antes
//pero con interesting graph
val interestingComponentCounts = sortedConnectedComponents(interesting.connectedComponents())
interestingComponentCounts.size
//Int = 18004

interestingComponentCounts.take(10).foreach(println)
//(-9217248190901527167,253) (-9167396012983474809,188)

//degrees of the most important vertices
val interestingDegrees = interesting.degrees.cache()

interestingDegrees.map(_._2).stats()
//(count: 21341, mean: 2,266342, stdev: 1,846737, max: 26,000000, min: 1,000000)

topNamesAndDegrees(interestingDegrees, doctorGraph).foreach(println)
// (A v Hardy,26) (D Philippides,22)


//Cliques and Clustering Coefficients
def avgClusteringCoef(graph: Graph[_, _]): Double = {
    val triCountGraph = graph.triangleCount()
    val maxTrisGraph = graph.degrees.mapValues(d => d * (d - 1) / 2.0)
    val clusterCoefGraph = triCountGraph.vertices.innerJoin(maxTrisGraph) {
      (vertexId, triCount, maxTris) => if (maxTris == 0) 0 else triCount / maxTris
    }
    clusterCoefGraph.map(_._2).sum() / graph.vertices.count()
  }

val avgCC = avgClusteringCoef(interesting)
// 0.30355015783830164

//Average Path Length with Pregel

  def mergeMaps(m1: Map[VertexId, Int], m2: Map[VertexId, Int]): Map[VertexId, Int] = {
    def minThatExists(k: VertexId): Int = {
      math.min(
        m1.getOrElse(k, Int.MaxValue),
        m2.getOrElse(k, Int.MaxValue))
    }

    (m1.keySet ++ m2.keySet).map {
      k => (k, minThatExists(k))
    }.toMap
  }
  
def update(id: VertexId, state: Map[VertexId, Int], msg: Map[VertexId, Int])
    : Map[VertexId, Int] = {
    mergeMaps(state, msg)
  }
  
  def checkIncrement(a: Map[VertexId, Int], b: Map[VertexId, Int], bid: VertexId)
    : Iterator[(VertexId, Map[VertexId, Int])] = {
    val aplus = a.map { case (v, d) => v -> (d + 1) }
    if (b != mergeMaps(aplus, b)) {
      Iterator((bid, aplus))
    } else {
      Iterator.empty
    }
  } 
  
  def iterate(e: EdgeTriplet[Map[VertexId, Int], _]): Iterator[(VertexId, Map[VertexId, Int])] = {
    checkIncrement(e.srcAttr, e.dstAttr, e.dstId) ++
    checkIncrement(e.dstAttr, e.srcAttr, e.srcId)
  }

//create method to compute avg path

def samplePathLengths[V, E](graph: Graph[V, E], fraction: Double = 0.02)
    : RDD[(VertexId, VertexId, Int)] = {
    val replacement = false
    val sample = graph.vertices.map(v => v._1).sample(
      replacement, fraction, 1729L)
    val ids = sample.collect().toSet

    val mapGraph = graph.mapVertices((id, v) => {
      if (ids.contains(id)) {
        Map(id -> 0)
      } else {
        Map[VertexId, Int]()
      }
    })

    val start = Map[VertexId, Int]()
    val res = mapGraph.ops.pregel(start)(update, iterate, mergeMaps)
    res.vertices.flatMap { case (id, m) =>
      m.map { case (k, v) =>
        if (id < k) {
          (id, k, v)
        } else {
          (k, id, v)
        }
      }
    }.distinct().cache()
  }
  
//calculate pahtlength and sampling 10% of the data 
val paths = samplePathLengths(interesting,0.10)

//show statistics
paths.map(_._3).filter(_ > 0).stats()

//show histogram
val hist = paths.map(_._3).countByValue()
hist.toSeq.sorted.foreach(println)













