

/*code to run
./bin/spark-shell --jars /Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/joda-time-2.9.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/nscala-time_2.10-2.6.0.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/ch09-risk-1.0.2.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/breeze-viz_2.10-0.9.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/breeze_2.10-0.9.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/jcommon-1.0.16.jar,/Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/jfreechart-1.0.13.jar
*/


import java.io.File
import java.text.SimpleDateFormat
import java.util.Locale

import scala.collection.mutable.ArrayBuffer
import scala.io.Source

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.rdd.RDD

import com.cloudera.datascience.risk.KernelDensity
import org.jfree.chart.axis.ValueAxis 
import breeze.linalg._ 
import breeze.plot._ 


import com.github.nscala_time.time.Implicits._

import org.apache.commons.math3.distribution.ChiSquaredDistribution
import org.apache.commons.math3.distribution.MultivariateNormalDistribution
import org.apache.commons.math3.random.MersenneTwister
import org.apache.commons.math3.stat.correlation.Covariance
import org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression

import org.joda.time.DateTime

def readInvestingDotComHistory(file: File): Array[(DateTime, Double)] = {
    val format = new SimpleDateFormat("MMM dd, yyyy",Locale.ENGLISH)
    val lines = Source.fromFile(file).getLines().toSeq
    lines.map(line => {
      val cols = line.split('\t')
      val date = new DateTime(format.parse(cols(0)))
      val value = cols(1).toDouble
      (date, value)
    }).reverse.toArray
  }
  
/**
   * the line "lines.tail.map" works to is useful for excluding the header row
   */
  def readYahooHistory(file: File): Array[(DateTime, Double)] = {
    val format = new SimpleDateFormat("yyyy-MM-dd")
    val lines = Source.fromFile(file).getLines().toSeq
    lines.tail.map(line => {
      val cols = line.split(',')
      val date = new DateTime(format.parse(cols(0)))
      val value = cols(1).toDouble
      (date, value)
    }).reverse.toArray
  }
  
  
//We load all the data
//and filter out instruments with less than five years of history

val start = new DateTime(2012, 11, 9, 0, 0)
val end = new DateTime(2015, 12, 9, 0, 0)
  
val files = new File("/Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/stocks/").listFiles()  


//se filtra para solo 4 años
val rawStocks = files.flatMap(file => {
  try {
    Some(readYahooHistory(file))
  } catch {
    case e: Exception => None
  }
}).filter(_.size >= 260*5+10)  
  
val factors1 = Array("OilPrice.tsv", "TresouryBond.tsv").
      map(x => new File("/Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/factors/" + x)).
      map(readInvestingDotComHistory)

val factors2 = Array("^GSPC.csv", "^IXIC.csv").
      map(x => new File("/Users/juandavid/Documents/GuideSparkJuanDavid/MonteCarlo/factors/" + x)).
      map(readYahooHistory)
      

/*
Colocar los datos para que comienzen siempre en la misma fecha y teminen
First, we need to trim all of our time series to the same region in time.
Then, we need to fill in missing values. To deal with time series that are missing values
at the start and end dates in the time region, we simply fill in those dates with
nearby values in the time region
*/
 def trimToRegion(history: Array[(DateTime, Double)], start: DateTime, end: DateTime): Array[(DateTime, Double)] = {
    var trimmed = history.dropWhile(_._1 < start).takeWhile(_._1 <= end)
    if (trimmed.head._1 != start) {
      trimmed = Array((start, trimmed.head._2)) ++ trimmed
    }
    if (trimmed.last._1 != end) {
      trimmed = trimmed ++ Array((end, trimmed.last._2))
    }
    trimmed
  }

/*
Crea un nuevo arraybuffer imputando fechas(al valor del dia anterior)
To deal with missing values within a time series, we use a simple imputation strategy
that fills in an instrument’s price as its most recent closing price before that day
*/
def fillInHistory(history: Array[(DateTime, Double)], start: DateTime, end: DateTime): Array[(DateTime, Double)] = {
    var cur = history
    val filled = new ArrayBuffer[(DateTime, Double)]()
    var curDate = start
    while (curDate < end) {
      if (cur.tail.nonEmpty && cur.tail.head._1 == curDate) {
        cur = cur.tail
      }

      filled += ((curDate, cur.head._2))

      curDate += 1.days
      // Skip weekends
      if (curDate.dayOfWeek().get > 5) curDate += 2.days
    }
    filled.toArray
  }

//apply functions
val stocks = rawStocks.map(trimToRegion(_, start, end)).map(fillInHistory(_, start, end))

val factors = (factors1 ++ factors2).map(trimToRegion(_, start, end)).map(fillInHistory(_, start, end))


def twoWeekReturns(history: Array[(DateTime, Double)]): Array[Double] = {
    history.sliding(10).map { window =>
      val next = window.last._2
      val prev = window.head._2
      (next - prev) / prev
    }.toArray
  }

/*
Recall that Value at Risk deals with losses over a particular time horizon. We are not
concerned with the absolute prices of instruments, but how those prices move over a
given length of time.
*/
val stockReturns = stocks.map(twoWeekReturns)
val factorReturns = factors.map(twoWeekReturns)

def plotDistributionArr(samples: Array[Double]): Figure = {
    val min = samples.min
    val max = samples.max
    // Using toList before toArray avoids a Scala bug
    val domain = Range.Double(min, max, (max - min) / 100).toList.toArray
    val densities = KernelDensity.estimate(samples, domain)
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain, densities)
    p.xlabel = "Two Week Return ($)"
    p.ylabel = "Density"
    f
  }

  def plotDistribution(samples: RDD[Double]): Figure = {
    val stats = samples.stats()
    val min = stats.min
    val max = stats.max
    // Using toList before toArray avoids a Scala bug
    val domain = Range.Double(min, max, (max - min) / 100).toList.toArray
    val densities = KernelDensity.estimate(samples, domain)
    val f = Figure()
    val p = f.subplot(0)
    p += plot(domain, densities)
    p.xlabel = "Two Week Return ($)"
    p.ylabel = "Density"
    f
  }

plotDistributionArr(factorReturns(2))
plotDistributionArr(factorReturns(3))



def factorMatrix(histories: Seq[Array[Double]]): Array[Array[Double]] = {
    val mat = new Array[Array[Double]](histories.head.length)
    for (i <- 0 until histories.head.length) {
      mat(i) = histories.map(_(i)).toArray
    }
    mat
  }

//val factorMat = factorMatrix(factorReturns)

def featurize(factorReturns: Array[Double]): Array[Double] = {
    val squaredReturns = factorReturns.map(x => math.signum(x) * x * x)
    val squareRootedReturns = factorReturns.map(x => math.signum(x) * math.sqrt(math.abs(x)))
    squaredReturns ++ squareRootedReturns ++ factorReturns
  }

//val factorFeatures = factorMat.map(featurize)

//create linear model
def linearModel(instrument: Array[Double], factorMatrix: Array[Array[Double]])
  : OLSMultipleLinearRegression = {
    val regression = new OLSMultipleLinearRegression()
    regression.newSampleData(instrument, factorMatrix)
    regression
  }

//use linear model
//val models = stockReturns.map(linearModel(_, factorFeatures))
//val factorWeights = models.map(_.estimateRegressionParameters()).toArray

/**
   * Calculate the return of a particular instrument under particular trial conditions.
   */
  def instrumentTrialReturn(instrument: Array[Double], trial: Array[Double]): Double = {
    var instrumentTrialReturn = instrument(0)
    var i = 0
    while (i < trial.length) {
      instrumentTrialReturn += trial(i) * instrument(i+1)
      i += 1
    }
    instrumentTrialReturn
  }

 /**
   * Calculate the full return of the portfolio under particular trial conditions.
   */
  def trialReturn(trial: Array[Double], instruments: Seq[Array[Double]]): Double = {
    var totalReturn = 0.0
    for (instrument <- instruments) {
      totalReturn += instrumentTrialReturn(instrument, trial)
    }
    totalReturn / instruments.size
  }
  
 def trialReturns(
      seed: Long,
      numTrials: Int,
      instruments: Seq[Array[Double]],
      factorMeans: Array[Double],
      factorCovariances: Array[Array[Double]]): Seq[Double] = {
    val rand = new MersenneTwister(seed)
    val multivariateNormal = new MultivariateNormalDistribution(rand, factorMeans,
      factorCovariances)

    val trialReturns = new Array[Double](numTrials)
    for (i <- 0 until numTrials) {
      val trialFactorReturns = multivariateNormal.sample()
      val trialFeatures = featurize(trialFactorReturns)
      trialReturns(i) = trialReturn(trialFeatures, instruments)
    }
    trialReturns
  }
  
  def computeFactorWeights(
      stocksReturns: Seq[Array[Double]],
      factorFeatures: Array[Array[Double]]): Array[Array[Double]] = {
    val models = stocksReturns.map(linearModel(_, factorFeatures))
    val factorWeights = Array.ofDim[Double](stocksReturns.length, factorFeatures.head.length+1)
    for (s <- 0 until stocksReturns.length) {
      factorWeights(s) = models(s).estimateRegressionParameters()
    }
    factorWeights
  }

def computeTrialReturns(
      stocksReturns: Seq[Array[Double]],
      factorsReturns: Seq[Array[Double]],
      sc: SparkContext,
      baseSeed: Long,
      numTrials: Int,
      parallelism: Int): RDD[Double] = {
    val factorMat = factorMatrix(factorsReturns)
    val factorCov = new Covariance(factorMat).getCovarianceMatrix().getData()
    val factorMeans = factorsReturns.map(factor => factor.sum / factor.size).toArray
    val factorFeatures = factorMat.map(featurize)
    val factorWeights = computeFactorWeights(stocksReturns, factorFeatures)

    val bInstruments = sc.broadcast(factorWeights)

    // Generate different seeds so that our simulations don't all end up with the same results
    val seeds = (baseSeed until baseSeed + parallelism)
    val seedRdd = sc.parallelize(seeds, parallelism)

    // Main computation: run simulations and compute aggregate return for each
    seedRdd.flatMap(
      trialReturns(_, numTrials / parallelism, bInstruments.value, factorMeans, factorCov))
  }
  
  //SIGUE LA EJECUCION
  
val numTrials = 10000000
val parallelism = 1000
val baseSeed = 1001L
val trials = computeTrialReturns(stockReturns, factorReturns, sc, baseSeed, numTrials,parallelism)
trials.cache()
  
//halla VaR y CVaR  
def fivePercentVaR(trials: RDD[Double]): Double = {
    val topLosses = trials.takeOrdered(math.max(trials.count().toInt / 20, 1))
    topLosses.last
  }

  def fivePercentCVaR(trials: RDD[Double]): Double = {
    val topLosses = trials.takeOrdered(math.max(trials.count().toInt / 20, 1))
    topLosses.sum / topLosses.length
  } 
  
val valueAtRisk = fivePercentVaR(trials)
val conditionalValueAtRisk = fivePercentCVaR(trials)
println("VaR 5%: " + valueAtRisk)
println("CVaR 5%: " + conditionalValueAtRisk)

//plot distribution trials
plotDistribution(trials)


//confidence intervals of cvar and var
def bootstrappedConfidenceInterval(
      trials: RDD[Double],
      computeStatistic: RDD[Double] => Double,
      numResamples: Int,
      pValue: Double): (Double, Double) = {
    val stats = (0 until numResamples).map { i =>
      val resample = trials.sample(true, 1.0)
      computeStatistic(resample)
    }.sorted
    val lowerIndex = (numResamples * pValue / 2 - 1).toInt
    val upperIndex = math.ceil(numResamples * (1 - pValue / 2)).toInt
    (stats(lowerIndex), stats(upperIndex))
  }
  
val varConfidenceInterval = bootstrappedConfidenceInterval(trials, fivePercentVaR, 100, .05)
val cvarConfidenceInterval = bootstrappedConfidenceInterval(trials, fivePercentCVaR, 100, .05)
println("VaR confidence interval: " + varConfidenceInterval)
println("CVaR confidence interval: " + cvarConfidenceInterval)

//p value de var 
def kupiecTestPValue(
      stocksReturns: Seq[Array[Double]],
      valueAtRisk: Double,
      confidenceLevel: Double): Double = {
    val failures = countFailures(stocksReturns, valueAtRisk)
    val total = stocksReturns(0).size
    val testStatistic = kupiecTestStatistic(total, failures, confidenceLevel)
    1 - new ChiSquaredDistribution(1.0).cumulativeProbability(testStatistic)
  }
  
println("Kupiec test p-value: " + kupiecTestPValue(stocksReturns, valueAtRisk, 0.05))

  
  
  
