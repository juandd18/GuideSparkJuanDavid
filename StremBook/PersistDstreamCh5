
///////////////
//spark shell sentence
spark-shell --master spark://quickstart.cloudera:7077 --jars /home/cloudera/Documents/ReadAccessLogScala.jar,/home/cloudera/Documents/spark-logs-analyzer_2.10-1.0.jar

/////////////////////

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import org.apache.spark.streaming.Duration
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream

//es el jar ReadAccessLogScala.jar
import com.juanScala.ApacheAccessLog 


//import com.databricks.apps.logs.ApacheAccessLog es otra clase que hace lo mismo
// es el jar spark-logs-analyzer_2.10-1.0.jar

import org.apache.spark.sql.SQLContext

//para utilizar DataFrames
val sqlContext = new SQLContext(sc)
import sqlContext.implicits._

val streamCtx = new StreamingContext(sc, Seconds(10))

val stream = FlumeUtils.createPollingStream(streamCtx, "10.0.2.15", 4951, StorageLevel.MEMORY_ONLY_SER_2)


//persist the Dstream en varias destinos
def persistsDstreams(dStream: DStream[SparkFlumeEvent],streamCtx: StreamingContext){
  
  val newDstream = stream.map { x => ApacheAccessLog.parseLogLine(new String(x.event.getBody().array())) }
  
  if (newDstream.count() == 0) {
        println("----------- NO ACCESS printing results ----------")
        println("No access data")
        println("----------- NO ACCESS printing results ----------")
      }
  
  
  //Writing Data as Text Files on Local File system
  // the method requares two input prefix and suffix
  // the final format is "<prefix><Milliseconds><suffix>"
  newDstream.saveAsTextFiles("/home/cloudera/Documents/outputDir/data-", "")
}


//
persistsDstreams(stream,streamCtx)


streamCtx.start()
