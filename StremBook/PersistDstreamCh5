
///////////////
//spark shell sentence
spark-shell --master spark://quickstart.cloudera:7077 --jars /home/cloudera/Documents/ReadAccessLogScala.jar,/home/cloudera/Documents/spark-logs-analyzer_2.10-1.0.jar

/////////////////////

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import org.apache.spark.streaming.Duration
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream


//es el jar ReadAccessLogScala.jar
import com.juanScala.ApacheAccessLog 


//import com.databricks.apps.logs.ApacheAccessLog es otra clase que hace lo mismo
// es el jar spark-logs-analyzer_2.10-1.0.jar

import org.apache.spark.sql.SQLContext

//para utilizar DataFrames
val sqlContext = new SQLContext(sc)
import sqlContext.implicits._

val streamCtx = new StreamingContext(sc, Seconds(10))

val stream = FlumeUtils.createPollingStream(streamCtx, "10.0.2.15", 4951, StorageLevel.MEMORY_ONLY_SER_2)


val newDstream = stream.map { x => ApacheAccessLog.parseLogLine(new String(x.event.getBody().array())) }


//persist the Dstream en varias destinos
def persistsDstreams(dStream: DStream[com.juanScala.ApacheAccessLog],streamCtx: StreamingContext){
  newDstream.foreachRDD(accessLogs => {
      if (accessLogs.count() == 0) {
        println("----------- NO ACCESS printing results ----------")
        println("No access com.databricks.app.logs received in this time interval")
        println("----------- NO ACCESS printing results ----------")
      } else {
        accessLogs.toDF().registerTempTable("logs")

        // mostrar todos los datos.
        
        println("----------- start printing results ----------")
        val contentSizeStats = sqlContext
          .sql("SELECT ipAddress, clientIdentd, userId, dateTime,method,endpoint,protocol,responseCode,contentSize FROM logs")
        contentSizeStats.map{dato => "ipAdrress: " + dato(0) + " clientIdent: " + dato(1) + 
            " userId: " + dato(2) + " dateTime: " + dato(3)+ " method: " + dato(4) + " endpoint: " + dato(5) +
            " protocol: " +dato(6) + " responseCode: " + dato(6) + " contentSize: " + dato(7)   }.collect().foreach(println)
          
        println("----------- finishing printing results ----------")
        
      }
    })
  //Writing Data as Text Files on Local File system
  // the method requares two input prefix and suffix
  // the final format is "<prefix><Milliseconds><suffix>"
  newDstream.saveAsTextFiles("/user/cloudera/streaming/textfiles/data-", "")
  
  //Defining saveAsObject for saving data in form of Hadoop Sequence Files
  newDstream.saveAsObjectFiles("/user/cloudera/streaming/sequenceFiles/data-")
  println("----------- FINAL ACCESS printing results ----------")
  
}

//
persistsDstreams(newDstream,streamCtx)



streamCtx.start()
