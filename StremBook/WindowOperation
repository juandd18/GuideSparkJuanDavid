

///////////////
//spark shell sentence
spark-shell --master spark://quickstart.cloudera:7077 --jars /home/cloudera/Documents/ReadAccessLogScala.jar,/home/cloudera/Documents/spark-logs-analyzer_2.10-1.0.jar

/////////////////////

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import org.apache.spark.streaming.Duration
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream

//es el jar ReadAccessLogScala.jar
import com.juanScala.ApacheAccessLog 


//import com.databricks.apps.logs.ApacheAccessLog es otra clase que hace lo mismo
// es el jar spark-logs-analyzer_2.10-1.0.jar

import org.apache.spark.sql.SQLContext

//para utilizar DataFrames
val sqlContext = new SQLContext(sc)
import sqlContext.implicits._

val streamCtx = new StreamingContext(sc, Seconds(5))

val stream = FlumeUtils.createPollingStream(streamCtx, "10.0.2.15", 4951, StorageLevel.MEMORY_ONLY_SER_2)

// window(windowLenght, slidingInterval) deben ser mulitplos del batch en este caso 5 segundos
//se multiplica porque se usan millisegundos
val WINDOW_LENGTH = new Duration(20 * 5000)
val SLIDE_INTERVAL = new Duration(10 * 5000)
val windowDStream = stream.window(WINDOW_LENGTH, SLIDE_INTERVAL)

//group by using having a solo los log stream que tengan el metodo GET
def executeGroupByDatos(dStream: DStream[SparkFlumeEvent],streamCtx: StreamingContext){
  
  val newDstream = stream.map { x => ApacheAccessLog.parseLogLine(new String(x.event.getBody().array())) }
  
  newDstream.foreachRDD(accessLogs => {
      if (accessLogs.count() == 0) {
        println("----------- NO ACCESS printing results ----------")
        println("No access com.databricks.app.logs received in this time interval")
        println("----------- NO ACCESS printing results ----------")
      } else {
        accessLogs.toDF().registerTempTable("logs")

        println("----------- start printing GROUP results ----------")
        val contentSizeStats = sqlContext
          .sql("SELECT ipAddress, COUNT(*) as total FROM logs GROUP BY ipAddress HAVING total > 1 ")
        contentSizeStats.map{dato => "------------------- ipAdrress: " + dato(0) + " Cuantos: " + dato(1)  }.collect().foreach(println)
          
        println("----------- finishing printing GROUP results ----------")
        
      }
    })

}


//cuantos veces una ip llega (usando group by) con mas de una vez
executeGroupByDatos(windowDStream,streamCtx)


streamCtx.start()

