
//flume-spark.conf (archivo de configuracion flume)
#create agent name a1
a1.sources = source1
a1.channels = channel1
a1.sinks = sink1


#configures source a1
a1.sources.source1.type = exec
a1.sources.source1.command = tail -F /opt/gen_logs/logs/access.log
a1.sources.source1.channels = channel1

#define a channel 
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 2000


#define sink utilizando avro
a1.sinks.sink1.type = avro
a1.sinks.sink1.hostname = 10.0.2.15
a1.sinks.sink1.port = 4951
a1.sinks.sink1.channel = channel1

///////////////////////////


import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream

val streamCtx = new StreamingContext(sc, Seconds(3))

val stream = FlumeUtils.createStream(streamCtx, "10.0.2.15", 4951, StorageLevel.MEMORY_ONLY_SER_2)

val outputStream = new ObjectOutputStream(Console.out)

def printValues(stream:DStream[SparkFlumeEvent],streamCtx: StreamingContext, outputStream: ObjectOutput){
    stream.foreachRDD(foreachFunc)
    //SparkFlumeEvent is the wrapper classes containing all the events captured by the Stream
    def foreachFunc = (rdd: RDD[SparkFlumeEvent]) => {
      val array = rdd.collect()
      println("---------Start Printing Results----------")
      println("Total size of Events= " +array.size)
      for(flumeEvent<-array){
        //This is to get the AvroFlumeEvent from SparkFlumeEvent 
        //for printing the Original Data
        val payLoad = flumeEvent.event.getBody()
        //Printing the actual events captured by the Stream
        println(new String(payLoad.array()))
      }
      println("---------Finished Printing Results----------")
    }
  }


//Most important statement which will initiate the Streaming Context
streamCtx.start();
//Wait till the execution is completed.
streamCtx.awaitTermination();


streamCtx.start()

////////////////////////
//flume-spark.conf (archivo de configuracion flume)
//utilizando org.apache.spark.streaming.flume.sink.SparkSink
//descargar los archivos de la siguente pagina http://spark.apache.org/docs/latest/streaming-flume-integration.html

#create agent name a1
a1.sources = source1
a1.channels = channel1
a1.sinks = sink1


#configures source a1
a1.sources.source1.type = netcat
a1.sources.source1.bind = 10.0.2.15
a1.sources.source1.port = 4949
a1.sources.source1.channels = channel1

#define a channel 
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 2000


#define sink
a1.sinks.sink1.type = org.apache.spark.streaming.flume.sink.SparkSink
a1.sinks.sink1.hostname = 10.0.2.15
a1.sinks.sink1.port = 4951
a1.sinks.sink1.channel = channel1



//version org.apache in flume

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream

val streamCtx = new StreamingContext(sc, Seconds(3))

val stream = FlumeUtils.createPollingStream(streamCtx, "10.0.2.15", 4951, StorageLevel.MEMORY_ONLY_SER_2)

//stream.count().map(cnt => "Received " + cnt + " flume events." ).print()

//stream.map(e => new String(e.event.getBody.array() ) ).print()

//stream.map(event => "Eventheader : header " +event.event.get(0).toString + " body " + new String(event.event.getBody.array)).print

val outputStream = new ObjectOutputStream(Console.out)

def printValues(stream:DStream[SparkFlumeEvent],streamCtx: StreamingContext, outputStream: ObjectOutput){
    stream.foreachRDD(foreachFunc)
    //SparkFlumeEvent is the wrapper classes containing all the events captured by the Stream
    def foreachFunc = (rdd: RDD[SparkFlumeEvent]) => {
      val array = rdd.collect()
      println("---------Start Printing Results----------")
      println("Total size of Events= " +array.size)
      for(flumeEvent<-array){
        //This is to get the AvroFlumeEvent from SparkFlumeEvent 
        //for printing the Original Data
        val payLoad = flumeEvent.event.getBody()
        //Printing the actual events captured by the Stream
        println(new String(payLoad.array()))
      }
      println("---------Finished Printing Results----------")
    }
  }


//Invoking custom Print Method for writing Events to Console
printValues(stream,streamCtx, outputStream)

streamCtx.start()











