
//flume file 
#create agent name a1
a1.sources = source1
a1.channels = channel1
a1.sinks = sink1


#configures source a1
a1.sources.source1.type = netcat
a1.sources.source1.bind = 10.0.2.15
a1.sources.source1.port = 4949
a1.sources.source1.channels = channel1

#define a channel 
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 2000


#define sink
a1.sinks.sink1.type = org.apache.spark.streaming.flume.sink.SparkSink
a1.sinks.sink1.hostname = 10.0.2.15
a1.sinks.sink1.port = 4951
a1.sinks.sink1.channel = channel1


///////////////////// create class serializable for run 
//in a flat map

import java.util.regex.Pattern
import java.util.regex.Matcher

case class ApacheAccessLog2(ipAddress: String, clientIdentd: String,
                           userId: String, dateTime: String, method: String,
                           endpoint: String, protocol: String,
                           responseCode: Int, contentSize: Long) extends Serializable {

}

object ApacheAccessLog extends Serializable {
  //val LOG_ENTRY_PATTERN = """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\d+)""".r
  val LOG_ENTRY_PATTERN = """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\S+)""";
  def parseLogLine(log: String): ApacheAccessLog2 = {
    
    val PATTERN = Pattern.compile(LOG_ENTRY_PATTERN);
    @transient val m = PATTERN.matcher(log);
    
    if (!m.find()) {
    System.out.println("Cannot parse logline" + log);
    }
    
    ApacheAccessLog2(m.group(1), m.group(2), m.group(3), m.group(4),m.group(5), m.group(6), m.group(7), m.group(8).toInt, m.group(9).toLong)
    //return  m.group(1)
  }
}

import java.util.regex.Pattern
import java.util.regex.Matcher

class ScalaLogAnalyzer extends Serializable{

def createDataMap(m:Matcher):Map[String,String] = {
  return Map[String, String](
  ("IP" -> m.group(1)),
  ("client" -> m.group(2)),
  ("user" -> m.group(3)),
  ("date" -> m.group(4)),
  ("method" -> m.group(5)),
  ("request" -> m.group(6)),
  ("protocol" -> m.group(7)),
  ("respCode" -> m.group(8)),
  ("size" -> m.group(9))
  )}

def tansfromLogData(logLine: String):Map[String,String] = {
  //Pattern which will extract the relevant data from Apache Access Log Files
  val LOG_ENTRY_PATTERN = """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\S+)""";
  val PATTERN = Pattern.compile(LOG_ENTRY_PATTERN);
  @transient val matcher = PATTERN.matcher(logLine);
  //Matching the pattern for the each line of the Apache access Log file
  if (!matcher.find()) {
    System.out.println("Cannot parse logline" + logLine);
  }
  //Finally create a Key/Value pair of extracted data and return to calling program
  createDataMap(matcher);
}

}
/////////////////////

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream

import com.juan.ApacheAccessLog._

import com.databricks.apps.logs.ApacheAccessLog
import org.apache.spark.sql.SQLContext

val sqlContext = new SQLContext(sc)
import sqlContext.implicits._

val streamCtx = new StreamingContext(sc, Seconds(5))

val stream = FlumeUtils.createPollingStream(streamCtx, "10.0.2.15", 4951, StorageLevel.MEMORY_ONLY_SER_2)

val newDstream = stream.map { x => ApacheAccessLog.parseLogLine(new String(x.event.getBody().array())) }


newDstream.foreachRDD(accessLogs => {
      if (accessLogs.count() == 0) {
        println("----------- NO ACCESS printing results ----------")
        println("No access com.databricks.app.logs received in this time interval")
        println("----------- start printing results ----------")
      } else {
        accessLogs.toDF().registerTempTable("logs")

        // Calculate statistics based on the content size.
        val contentSizeStats = sqlContext
          .sql("SELECT SUM(contentSize), COUNT(*), MIN(contentSize), MAX(contentSize) FROM logs")
          .first()
        
        println("----------- start printing results ----------")
        println("Content Size Avg: %s, Min: %s, Max: %s".format(
          contentSizeStats.getLong(0) / contentSizeStats.getLong(1),
          contentSizeStats(2),
          contentSizeStats(3)))
        println("----------- finishing printing results ----------")

        
      }
    })



//print the values of the Dstream
def printLogvalues(stream: DStream[SparkFlumeEvent],streamCtx: StreamingContext){

  def foreachFunc = (rdd: RDD[SparkFlumeEvent]) => {
  val PATTERN = """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\d+)""".r
  val array = rdd.collect()
  println("----------- start printing results ----------")
  for(flumeEvent <- array.array){
    
    val payLoad = flumeEvent.event.getBody()
    val logline = new String(payLoad.array())
    val res = PATTERN.findFirstMatchIn(logline)
    if (res.isEmpty) {
      println("Cannot parse log line: " + logline)
    }
    val m = res.get
    
    print("ID "  m.group(1)," date ",m.group(2))
  }
  println("----------- finishing printing results ----------")
  }
  
  //use foreach function
  stream.foreachRDD(foreachFunc)
}


def executeTransformation(dStream: DStream[SparkFlumeEvent],streamCtx: StreamingContext){
  //print values
  //printLogvalues(dStream,streamCtx)
}

def executeTransformation2(dStream: DStream[String],streamCtx: StreamingContext){
  println("----------- start printing results ----------")
  val valores = dStream.map(x => x.split(' '))
  for(valor <- valores){
    print(valor)
  }
  println("----------- finishing printing results ----------")
}
 

//executeTransformation(stream,streamCtx)
executeTransformation2(listado,streamCtx)

streamCtx.start()





















