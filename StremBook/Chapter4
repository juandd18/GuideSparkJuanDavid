
//flume file 
#create agent name a1
a1.sources = source1
a1.channels = channel1
a1.sinks = sink1


#configures source a1
a1.sources.source1.type = netcat
a1.sources.source1.bind = 10.0.2.15
a1.sources.source1.port = 4949
a1.sources.source1.channels = channel1

#define a channel 
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 2000


#define sink
a1.sinks.sink1.type = org.apache.spark.streaming.flume.sink.SparkSink
a1.sinks.sink1.hostname = 10.0.2.15
a1.sinks.sink1.port = 4951
a1.sinks.sink1.channel = channel1


///////////////////// create class serializable for run 
//in a flat map

case class ApacheAccessLog2(ipAddress: String, clientIdentd: String,
                           userId: String, dateTime: String, method: String,
                           endpoint: String, protocol: String,
                           responseCode: Int, contentSize: Long) extends Serializable {

}

object ApacheAccessLog extends Serializable {
  val PATTERN = """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\d+)""".r

  def parseLogLine(log: String): ApacheAccessLog2 = {
    val res = PATTERN.findFirstMatchIn(log)
    if (res.isEmpty) {
      throw new RuntimeException("Cannot parse log line: " + log)
    }
    val m = res.get
    //ApacheAccessLog2(m.group(1), m.group(2), m.group(3), m.group(4),m.group(5), m.group(6), m.group(7), m.group(8).toInt, m.group(9).toLong)
    return  m.group(1)
  }
}

/////////////////////

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream

case class ApacheAccessLog(ipAddress: String, clientIdentd: String,
                           userId: String, dateTime: String, method: String,
                           endpoint: String, protocol: String,
                           responseCode: Int, contentSize: Long) extends Serializable {

}
 
val streamCtx = new StreamingContext(sc, Seconds(3))

val stream = FlumeUtils.createPollingStream(streamCtx, "10.0.2.15", 4951, StorageLevel.MEMORY_ONLY_SER_2)

val listado = stream.map(x => new String(x.event.getBody().array()))

//print the values of the Dstream
def printLogvalues(stream: DStream[SparkFlumeEvent],streamCtx: StreamingContext){

  def foreachFunc = (rdd: RDD[SparkFlumeEvent]) => {
  val PATTERN = """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\d+)""".r
  val array = rdd.collect()
  println("----------- start printing results ----------")
  for(flumeEvent <- array.array){
    
    val payLoad = flumeEvent.event.getBody()
    val logline = new String(payLoad.array())
    val res = PATTERN.findFirstMatchIn(logline)
    if (res.isEmpty) {
      println("Cannot parse log line: " + logline)
    }
    val m = res.get
    
    print("ID "  m.group(1)," date ",m.group(2))
  }
  println("----------- finishing printing results ----------")
  }
  
  //use foreach function
  stream.foreachRDD(foreachFunc)
}


def executeTransformation(dStream: DStream[SparkFlumeEvent],streamCtx: StreamingContext){
  //print values
  //printLogvalues(dStream,streamCtx)
}

def executeTransformation2(dStream: DStream[String],streamCtx: StreamingContext){
  println("----------- start printing results ----------")
  val valores = dStream.map(x => x.split(' '))
  for(valor <- valores){
    print(valor)
  }
  println("----------- finishing printing results ----------")
}
 

//executeTransformation(stream,streamCtx)
executeTransformation2(listado,streamCtx)

streamCtx.start()





















