
//flume file 
#create agent name a1
a1.sources = source1
a1.channels = channel1
a1.sinks = sink1


#configures source a1
a1.sources.source1.type = netcat
a1.sources.source1.bind = 10.0.2.15
a1.sources.source1.port = 4949
a1.sources.source1.channels = channel1

#define a channel 
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 2000


#define sink
a1.sinks.sink1.type = org.apache.spark.streaming.flume.sink.SparkSink
a1.sinks.sink1.hostname = 10.0.2.15
a1.sinks.sink1.port = 4951
a1.sinks.sink1.channel = channel1


///////////////////// create scala object in eclipse luna (which allows create scala object and projects) 
//for read the apache acces log

package com.juanScala

case class ApacheAccessLog(ipAddress: String, clientIdentd: String,
                           userId: String, dateTime: String, method: String,
                           endpoint: String, protocol: String,
                           responseCode: Int, contentSize: Long) {

}

object ApacheAccessLog {
  val PATTERN = """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\d+)""".r

  def parseLogLine(log: String): ApacheAccessLog = {
    val res = PATTERN.findFirstMatchIn(log)
    if (res.isEmpty) {
      throw new RuntimeException("Cannot parse log line: " + log)
    }
    val m = res.get
    ApacheAccessLog(m.group(1), m.group(2), m.group(3), m.group(4),
      m.group(5), m.group(6), m.group(7), m.group(8).toInt, m.group(9).toLong)
  }
}

///////////////
//spark shell sentence
spark-shell --master spark://quickstart.cloudera:7077 --jars /home/cloudera/Documents/ReadAccessLogScala.jar,/home/cloudera/Documents/spark-logs-analyzer_2.10-1.0.jar

/////////////////////

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream

//es el jar ReadAccessLogScala.jar
import com.juanScala.ApacheAccessLog 


//import com.databricks.apps.logs.ApacheAccessLog es otra clase que hace lo mismo
// es el jar spark-logs-analyzer_2.10-1.0.jar

import org.apache.spark.sql.SQLContext

//para utilizar DataFrames
val sqlContext = new SQLContext(sc)
import sqlContext.implicits._

val streamCtx = new StreamingContext(sc, Seconds(5))

val stream = FlumeUtils.createPollingStream(streamCtx, "10.0.2.15", 4951, StorageLevel.MEMORY_ONLY_SER_2)

def executeTransformation(dStream: DStream[SparkFlumeEvent],streamCtx: StreamingContext){
  
  val newDstream = stream.map { x => ApacheAccessLog.parseLogLine(new String(x.event.getBody().array())) }
  
  newDstream.foreachRDD(accessLogs => {
      if (accessLogs.count() == 0) {
        println("----------- NO ACCESS printing results ----------")
        println("No access com.databricks.app.logs received in this time interval")
        println("----------- start printing results ----------")
      } else {
        accessLogs.toDF().registerTempTable("logs")

        // Calculate statistics based on the content size.
        val contentSizeStats = sqlContext
          .sql("SELECT SUM(contentSize), COUNT(*), MIN(contentSize), MAX(contentSize) FROM logs")
          .first()
        
        println("----------- start printing results ----------")
        println("Content Size Avg: %s, Min: %s, Max: %s".format(
          contentSizeStats.getLong(0) / contentSizeStats.getLong(1),
          contentSizeStats(2),
          contentSizeStats(3)))
        println("----------- finishing printing results ----------")

        
      }
    })

  
}

//primer punto mostar avg, min, max
executeTransformation(stream,streamCtx)



streamCtx.start()





















